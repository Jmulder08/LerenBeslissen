{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z5YVNCz9mLL"
   },
   "source": [
    "# AIMET dependencies install & build\n",
    "The following group of cells installs the AIMET library for you. For more details, please see [this link](https://github.com/quic/aimet/blob/develop/packaging/google_colab_install.md).\n",
    "\n",
    "You can clone this notebook and use it in your own project. Make sure that before running these cells, you connect to a hosted environment with a GPU accelerator. (Runtime -> Change runtime -> Hardware Accelerator(GPU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Hz_C0-x_hbw"
   },
   "source": [
    "## Installing dependencies\n",
    "May prompt you.\n",
    "# Ignore first 4 code cells when running in docker!!!\n",
    "## Start from \"Train a model on MNIST data\" :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 832825,
     "status": "ok",
     "timestamp": 1610296563283,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "xCMVKw4N9lYv",
    "outputId": "979c56ce-080f-4e3c-e056-9b77d6ad24e6"
   },
   "source": [
    "!pip3 uninstall --yes protobuf\n",
    "!pip3 uninstall --yes tensorflow\n",
    "!apt-get update\n",
    "!apt-get install python3.6\n",
    "!apt-get install python3-dev\n",
    "!apt-get install python3-pip\n",
    "!apt-get install liblapacke liblapacke-dev\n",
    "!apt-get install wget\n",
    "!pip3 install numpy==1.16.4\n",
    "!apt-get install libgtest-dev build-essential cmake\n",
    "!pip3 --no-cache-dir install opencv-python==4.1.0.25\n",
    "!pip3 --no-cache-dir install pillow==6.2.1\n",
    "!pip3 install pytorch-ignite==0.1.0\n",
    "!wget -q https://github.com/Itseez/opencv/archive/3.1.0.tar.gz -O /tmp/3.1.0.tar.gz > /dev/null\n",
    "!tar -C /tmp -xvf /tmp/3.1.0.tar.gz > /dev/null\n",
    "%cd /tmp/opencv-3.1.0\n",
    "%mkdir release\n",
    "%cd release\n",
    "!cmake -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DBUILD_SHARED_LIBS=OFF -DCMAKE_BUILD_TYPE=release -DWITH_FFMPEG=OFF -DBUILD_TESTS=OFF -DWITH_CUDA=OFF -DBUILD_PERF_TESTS=OFF -DWITH_IPP=OFF -DENABLE_PRECOMPILED_HEADERS=OFF .. > /dev/null\n",
    "!make -j16 > /dev/null\n",
    "!make -j16 install > /dev/null\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
    "!apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
    "!dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
    "!apt-get update\n",
    "!wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
    "!apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
    "!apt-get update\n",
    "!apt install cuda-cublas-10-0 cuda-cufft-10-0 cuda-curand-10-0 cuda-cusolver-10-0\n",
    "!apt-get update && apt install cuda-cusparse-10-0 libcudnn7=7.6.2.24-1+cuda10.0 libnccl2=2.4.8-1+cuda10.0  cuda-command-line-tools-10.0\n",
    "!pip3 install scipy==1.1.0\n",
    "!pip3 install protobuf==3.7.1\n",
    "!pip3 install scikit-learn==0.19.1\n",
    "!pip3 install tb-nightly==1.14.0a20190517\n",
    "!pip3 install tensorboardX==1.7\n",
    "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.5.0%2Bcu100-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install tensorflow-gpu==1.15.0\n",
    "!pip3 install future==0.17.1\n",
    "!pip3 install tensorboard==1.14\n",
    "!pip3 install bokeh==1.2.0\n",
    "!pip3 install pandas==0.22.0\n",
    "!pip3 install holoviews==1.12.7\n",
    "!pip3 install --no-deps bokeh==1.2.0 hvplot==0.4.0\n",
    "!pip3 install jsonschema==3.1.1\n",
    "!pip3 install osqp onnx\n",
    "\n",
    "!ln -s /usr/local/cuda-10.0 /usr/local/cuda\n",
    "!apt-get update && apt-get install -y libjpeg8-dev\n",
    "!ln -s /usr/lib/x86_64-linux-gnu/libjpeg.so /usr/lib\n",
    "\n",
    "!apt install zlib1g-dev\n",
    "\n",
    "!pip3 uninstall --yes Pillow && pip3 install Pillow-SIMD==6.0.0.post0\n",
    "!pip3 uninstall --yes pytest\n",
    "!pip3 install pytest\n",
    "!pip3 install setuptools==41.0.1\n",
    "!pip3 install keras==2.2.4\n",
    "\n",
    "%rm -rf /usr/local/bin/python\n",
    "!ln -s /usr/bin/python3 /usr/local/bin/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiiZL_wi_oeP"
   },
   "source": [
    "After installing the dependencies, you must restart the environment before proceeding. (Runtime -> Restart Runtime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdF9TK03CKiW"
   },
   "source": [
    "## AIMET build and installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115891,
     "status": "ok",
     "timestamp": 1610296813311,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "1nR1MrsI-EW1",
    "outputId": "03b22e94-4bbf-499e-c364-df5b131f2975",
    "scrolled": true
   },
   "source": [
    "%cd /content/\n",
    "!rm -rf aimet_code\n",
    "!mkdir aimet_code\n",
    "%cd aimet_code\n",
    "!git clone https://github.com/quic/aimet.git\n",
    "%cd aimet\n",
    "%mkdir -p ./ThirdParty/googletest\n",
    "%pushd ./ThirdParty/googletest\n",
    "!git clone https://github.com/google/googletest.git -b release-1.8.0 googletest-release-1.8.0\n",
    "%popd\n",
    "%cd /content/aimet_code\n",
    "%mkdir build\n",
    "%cd build\n",
    "!cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON ../aimet\n",
    "!make -j 8\n",
    "!make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6qARmUzAZj4"
   },
   "source": [
    "## Setting up `PYTHONPATH` and `LD_LIBRARY_PATH`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INEc0N5xAbuz"
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r'/content/aimet_code/build/staging/universal/lib/python')\n",
    "sys.path.append(r'/content/aimet_code/build/staging/universal/lib/x86_64-linux-gnu')\n",
    "sys.path.append(r'/usr/local/lib/python3.6/dist-packages')\n",
    "sys.path.append(r'/content/aimet_code/build/artifacts')\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['LD_LIBRARY_PATH']+= \":/content/aimet_code/build/artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DiI-1hvAhd3"
   },
   "source": [
    "## Run unit tests\n",
    "If the installation went smoothly, all tests should pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 438596,
     "status": "ok",
     "timestamp": 1610297270113,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "WhO4FvxHAijc",
    "outputId": "fdf0203b-f52d-4e6f-f89f-722b155fcabf"
   },
   "source": [
    "%cd /content/aimet_code/build/\n",
    "!ctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZJ8R7fCRx_f"
   },
   "source": [
    "# Train a model on MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXn0f41ueDFN"
   },
   "source": [
    "Set random seed for reprodubicility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1610306425865,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "Y-GPWZa2eFS1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_grayscale(tensor):\n",
    "    plt.imshow(tensor, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_zQ5VelR30T"
   },
   "source": [
    "Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1610306428470,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "z9-dFvJhRxsf"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5(torch.nn.Module):          \n",
    "\n",
    "    def __init__(self):     \n",
    "        super(LeNet5, self).__init__()\n",
    "        self.convs = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2),\n",
    "                                    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding=0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2)\n",
    "                                  )\n",
    "\n",
    "        self.linears = nn.Sequential(nn.Linear(16*5*5, 120),\n",
    "                                     nn.Linear(120, 84),\n",
    "                                     nn.Linear(84, 10)\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.linears(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for creating imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms\n",
    "\n",
    "class MNIST_Imbalancer:\n",
    "    ''' class around MNIST object for applying imbalance to its dataset '''\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Convert imgs to tensor and normalize by mean and stddev of the training set\n",
    "        transformImg = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        \n",
    "        \n",
    "        self.dataset = MNIST(root='./data', train=True, download=True, transform=transformImg)\n",
    "        self.class_order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] #MNIST-specific\n",
    "        self.selection_dict = dict()\n",
    "        self.shuffle()\n",
    "    \n",
    "    \n",
    "    def shuffle(self):\n",
    "        ''' shuffle dataset '''\n",
    "        # generate random ordered indeces for dataset\n",
    "        datapoints = self.dataset.data.shape[0]\n",
    "        rand_idx = torch.randperm(datapoints)\n",
    "\n",
    "        # shuffle data and targets in the same way\n",
    "        self.dataset.data = self.dataset.data[rand_idx]\n",
    "        self.dataset.targets = self.dataset.targets[rand_idx]\n",
    "    \n",
    "    \n",
    "\n",
    "    def keep_selection(self, target, selection):\n",
    "        ''' create imbalance in single class of a dataset '''\n",
    "\n",
    "        # get indices of imgs of target number and remove selection of the indices\n",
    "        target_mask = self.dataset.targets == target\n",
    "        selection_idx = target_mask.nonzero()[round(len(target_mask.nonzero())*selection):]\n",
    "\n",
    "        # make mask wich selects all data except for indices in selection\n",
    "        selection_mask = np.ones(len(self.dataset.data), dtype=bool)\n",
    "        selection_mask[selection_idx] = False\n",
    "\n",
    "        # apply mask to remove the selected data\n",
    "        self.dataset.data = self.dataset.data[selection_mask]\n",
    "        self.dataset.targets = self.dataset.targets[selection_mask]\n",
    "\n",
    "    \n",
    "    \n",
    "    def apply_imbalance(self):\n",
    "        ''' create imbalance in dataset according to selection dict '''\n",
    "\n",
    "        # throw away a part of the data for each class\n",
    "        for class_number, selection in self.selection_dict.items():\n",
    "            self.keep_selection(class_number, selection)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def linear_imbalance(self, rho, apply=True):\n",
    "        ''' create selection dict with linear imbalance '''\n",
    "        \n",
    "        min_examples = 1 / rho\n",
    "        \n",
    "        n_steps = len(self.class_order) - 1\n",
    "        linear_step = (1.0 - min_examples) / n_steps\n",
    "\n",
    "        # interpolate the classes between the minimum and maximum linearly\n",
    "        for i, data_class in enumerate(reversed(self.class_order)):\n",
    "            self.selection_dict[data_class] = min_examples + (i * linear_step)\n",
    "        \n",
    "        if apply:\n",
    "            self.apply_imbalance()\n",
    "\n",
    "        return self.selection_dict\n",
    "\n",
    "            \n",
    "            \n",
    "    def step_imbalance(self, rho, mu, apply=True):\n",
    "        ''' create selection dict with step imbalance '''\n",
    "        \n",
    "        min_examples = 1 / rho\n",
    "        \n",
    "        \n",
    "        n_classes = len(self.class_order)\n",
    "        step_index = int(mu * n_classes)\n",
    "\n",
    "        for i, data_class in enumerate(reversed(self.class_order)):\n",
    "            if i < step_index:\n",
    "                self.selection_dict[data_class] = min_examples\n",
    "            else:\n",
    "                self.selection_dict[data_class] = 1.0\n",
    "\n",
    "        if apply:\n",
    "            self.apply_imbalance()\n",
    "            \n",
    "        return self.selection_dict\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    def long_tailed_imbalance(self, mu, apply=True):\n",
    "        ''' create selection dict with long-tailed imbalance'''\n",
    "        \n",
    "        max_index = len(self.class_order) - 1\n",
    "        \n",
    "        # set selection for each class according to long-tailed function, mu is in (0,1)\n",
    "        for i, data_class in enumerate(self.class_order):\n",
    "            self.selection_dict[data_class] = mu**i\n",
    "\n",
    "        if apply:\n",
    "            self.apply_imbalance()\n",
    "        \n",
    "        return self.selection_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    def plot(self, title, color='r'):\n",
    "        ''' plot the imbalance created by an imbalance function '''\n",
    "        \n",
    "        plt.bar(self.selection_dict.keys(), self.selection_dict.values(), color=color)\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484,
     "referenced_widgets": [
      "2328a7b009d44c5e9eacff2a755cbebb",
      "ed9207ce942047e9ae6072508f523741",
      "46c25a22208a49eda4e6eab32d4eaf2f",
      "d4ff65d7c61d4bf9b73c03e29c28a8ad",
      "2a5ab588d2f24b0ab3ec8ce388f5127f",
      "a5f8150fe3124fc88dc714b404006f34",
      "8da6d53cc36743e6bd5058b8840e0c41",
      "f6769ba59c21406e8c259661bdbf68f1",
      "4b07de9e194f4439baeae01257e4ba52",
      "bd10cc47f292451d922d8980f5e93ddf",
      "7989862673284a799c9d51e431373247",
      "154e97cc69984a0ab11ced26576ebbd7",
      "5503b3b0dbb548589aaa0279b976a414",
      "efa8ccf950004549a00250f6332bc16b",
      "c41636ad0caf4621870bd12e89518604",
      "ab3eb90c487746e8b65e62d7757c0e78",
      "af3a484183a24008a6d38960f73ffb3e",
      "ac82f07b477b4502b21ebaa8a539c4a3",
      "012c3c9adc694baba6feb904d60f9201",
      "8ce0005792b74005a30cb426b5c8077a",
      "0b0a62dffc4341069d860e1e7c09a85c",
      "3b5ccd5354b040b58f645351f4d950f3",
      "543c9bdec83f486eaa8b9b2b58ef211a",
      "9fffcd6e9d7a449fb4bfccedc6cb70a1",
      "df98becb207b42f2bbeaf23708e5be02",
      "15b6b7d213c94345b6dedf59e8e065dc",
      "6126bcc00c54407688d7d3a081fa43c0",
      "15c6e5135ed849bc89dec5bf0a38363b",
      "657579abb9c4488c8f1a810a679dfda9",
      "1af67330e5d947718a1e80e982898052",
      "c9dc0f6d46b74558bcb4962e3f1fb489",
      "ab99643118ed466298c1ea12d4ac53d9"
     ]
    },
    "executionInfo": {
     "elapsed": 2453,
     "status": "ok",
     "timestamp": 1610306438151,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "kH5o32FLVJ2D",
    "outputId": "49acbac6-c0ea-4f76-b479-d7bf33ecacfb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert imgs to tensor and normalize by mean and stddev of the training set\n",
    "transformImg = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "test = MNIST(root='./data', train=False, download=True, transform=transformImg)\n",
    "\n",
    "train_imbalance = MNIST_Imbalancer()\n",
    "\n",
    "train_imbalance.linear_imbalance(2, apply=False)\n",
    "train_imbalance.plot(\"Linear Imbalance\")\n",
    "\n",
    "#train_imbalance.step_imbalance(4, 0.5, apply=False)\n",
    "#train_imbalance.plot(\"Step Imbalance\")\n",
    "\n",
    "#train_imbalance.long_tailed_imbalance(0.7)\n",
    "#train_imbalance.plot(\"Long-Tailed Imbalance\")\n",
    "\n",
    "train = train_imbalance.dataset\n",
    "\n",
    "# Define train/test loaders\n",
    "train_loader = DataLoader(train, batch_size=128, num_workers=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size=1024, num_workers=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iRixegDe7Ot"
   },
   "source": [
    "Define functions for conducting the training and testing epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1610306455425,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "BgDcuZVhe9C4"
   },
   "outputs": [],
   "source": [
    "def accuracy(out, y):\n",
    "    preds = out.argmax(dim=1, keepdim=True).squeeze()\n",
    "    correct = preds.eq(y).sum().item()\n",
    "    return correct\n",
    "\n",
    "def train_epoch(model, opt, train_loader, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    n_samples = 0\n",
    "    for x,y in train_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model.forward(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        epoch_acc += accuracy(out, y)\n",
    "        epoch_loss += loss.item()\n",
    "        n_samples += x.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    epoch_acc = epoch_acc / n_samples\n",
    "    epoch_loss = epoch_loss / n_samples\n",
    "\n",
    "    return epoch_acc, epoch_loss\n",
    "\n",
    "def test_epoch(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model.forward(x)\n",
    "            loss = criterion(out, y)\n",
    "\n",
    "            epoch_acc += accuracy(out, y)\n",
    "            epoch_loss += loss.item()\n",
    "            n_samples += x.size(0)\n",
    "\n",
    "    epoch_acc = epoch_acc / n_samples\n",
    "    epoch_loss = epoch_loss / n_samples\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w97dpw3dyde"
   },
   "source": [
    "Instantiate required objects and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D28i0tp6VqTZ"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# to use for weighted loss functions, values should be according to imbalance\n",
    "weights = torch.FloatTensor([1,1,1,1,1,1,2,1,1,2])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epochs = 10\n",
    "writer = SummaryWriter()\n",
    "model = LeNet5().to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Train+test, log to tensorboard\n",
    "# It's recommended to also print all the scalar values\n",
    "for i in range(n_epochs):\n",
    "    train_acc, train_loss = train_epoch(model, opt, train_loader, criterion, device)\n",
    "    test_acc, test_loss = test_epoch(model, test_loader, criterion, device)\n",
    "    writer.add_scalar('train/acc', train_acc, i+1)\n",
    "    writer.add_scalar('train/loss', train_loss, i+1)\n",
    "    writer.add_scalar('test/acc', test_acc, i+1)\n",
    "    writer.add_scalar('test/loss', test_loss, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW_XJaFxlWFk"
   },
   "source": [
    "Let's check out how the model did with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 5154,
     "status": "ok",
     "timestamp": 1610307800543,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "A3fkVkHYfMi9",
    "outputId": "9088161d-a432-4182-ff53-81f7fbd8454a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Requirement already satisfied: tensorboard-plugin-wit in /usr/local/lib/python3.6/dist-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13031), started 0:01:25 ago. (Use '!kill 13031' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d9e50bcdc6c53b1c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d9e50bcdc6c53b1c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "!pip3 install tensorboard-plugin-wit\n",
    "%tensorboard --logdir ./runs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p7dzY73mfBQ"
   },
   "source": [
    "# Do channel pruning with AIMET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFFOWmSmmh0H"
   },
   "source": [
    "Import necessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4679,
     "status": "ok",
     "timestamp": 1610297563542,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "mE-CvbnmmhMl",
    "outputId": "0fae64dd-1031-4c55-c2d1-63293f2e3ed6"
   },
   "outputs": [],
   "source": [
    "from aimet_common.defs import CostMetric, CompressionScheme, GreedySelectionParameters\n",
    "from aimet_torch.defs import ChannelPruningParameters, SpatialSvdParameters, ModuleCompRatioPair\n",
    "from aimet_torch.compress import ModelCompressor\n",
    "from aimet_torch.onnx_utils import OnnxSaver\n",
    "from decimal import Decimal\n",
    "\n",
    "# Model compressor needs an evaluation function with this specific signature\n",
    "def eval_callback(model, iterations, use_cuda=True):\n",
    "    model.eval()\n",
    "    epoch_acc = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for idx,(x,y) in enumerate(test_loader):\n",
    "            if use_cuda:\n",
    "                x, y = x.to('cuda:0'), y.to('cuda:0')\n",
    "\n",
    "            out = model.forward(x)\n",
    "            epoch_acc += accuracy(out, y)\n",
    "            n_samples += x.size(0)\n",
    "\n",
    "            if iterations is not None:\n",
    "                if idx == iterations:\n",
    "                    break\n",
    "        epoch_acc = epoch_acc / n_samples\n",
    "\n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCSBR_ByrT4z"
   },
   "source": [
    "Do the actual pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83306,
     "status": "ok",
     "timestamp": 1610297656200,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "N75lsRaAmna1",
    "outputId": "c3ff1dcd-ae7c-4b05-888b-01ff888a993f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:24,524 - CompRatioSelect - INFO - Analyzing compression ratio: 0.1 =====================>\n",
      "2021-01-13 07:49:24,559 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:24,797 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:25,791 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.100000 ==> eval_score=0.954571\n",
      "2021-01-13 07:49:25,792 - CompRatioSelect - INFO - Analyzing compression ratio: 0.2 =====================>\n",
      "2021-01-13 07:49:25,817 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:26,025 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:27,191 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.200000 ==> eval_score=0.955178\n",
      "2021-01-13 07:49:27,192 - CompRatioSelect - INFO - Analyzing compression ratio: 0.3 =====================>\n",
      "2021-01-13 07:49:27,215 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:27,434 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:28,544 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.300000 ==> eval_score=0.954672\n",
      "2021-01-13 07:49:28,545 - CompRatioSelect - INFO - Analyzing compression ratio: 0.4 =====================>\n",
      "2021-01-13 07:49:28,568 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:28,809 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:29,894 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.400000 ==> eval_score=0.966560\n",
      "2021-01-13 07:49:29,895 - CompRatioSelect - INFO - Analyzing compression ratio: 0.5 =====================>\n",
      "2021-01-13 07:49:29,924 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:30,163 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:31,297 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.500000 ==> eval_score=0.966611\n",
      "2021-01-13 07:49:31,298 - CompRatioSelect - INFO - Analyzing compression ratio: 0.6 =====================>\n",
      "2021-01-13 07:49:31,334 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:31,603 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:33,145 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.600000 ==> eval_score=0.966307\n",
      "2021-01-13 07:49:33,146 - CompRatioSelect - INFO - Analyzing compression ratio: 0.7 =====================>\n",
      "2021-01-13 07:49:33,187 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:33,464 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:35,168 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.700000 ==> eval_score=0.966712\n",
      "2021-01-13 07:49:35,169 - CompRatioSelect - INFO - Analyzing compression ratio: 0.8 =====================>\n",
      "2021-01-13 07:49:35,206 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:35,494 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:37,298 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.800000 ==> eval_score=0.966662\n",
      "2021-01-13 07:49:37,300 - CompRatioSelect - INFO - Analyzing compression ratio: 0.9 =====================>\n",
      "2021-01-13 07:49:37,334 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:37,656 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 07:49:39,401 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.900000 ==> eval_score=0.966965\n",
      "2021-01-13 07:49:39,403 - CompRatioSelect - INFO - Greedy selection: Saved eval dict to ./data/greedy_selection_eval_scores_dict.pkl\n",
      "2021-01-13 07:49:39,407 - CompRatioSelect - INFO - Greedy selection: overall_min_score=0.954571, overall_max_score=0.966965\n",
      "2021-01-13 07:49:39,408 - CompRatioSelect - INFO - Greedy selection: Original model cost=(Cost: memory=61470, mac=416520)\n",
      "2021-01-13 07:49:39,443 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:39,511 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:39,577 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:39,644 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 07:49:39,709 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:39,770 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:39,829 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:39,889 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:39,949 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,010 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,136 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,212 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,278 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,341 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,400 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,460 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,494 - CompRatioSelect - INFO - Greedy selection: final choice - comp_ratio=0.427639, score=0.966307\n",
      "2021-01-13 07:49:40,520 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 07:49:40,749 - ChannelPruning - INFO - finished linear regression fit \n"
     ]
    }
   ],
   "source": [
    "greedy_params = GreedySelectionParameters(target_comp_ratio=Decimal(0.5))\n",
    "# Exclude first layer from pruning\n",
    "modules_to_ignore = [model.convs[0]]\n",
    "auto_params = ChannelPruningParameters.AutoModeParams(greedy_params, modules_to_ignore)\n",
    "input_shape = (1, 1, 28, 28)\n",
    "channel_pruning_parameters = ChannelPruningParameters(mode=ChannelPruningParameters.Mode.auto,\n",
    "                                                      params=auto_params,\n",
    "                                                      data_loader=train_loader,\n",
    "                                                      num_reconstruction_samples=1024,\n",
    "                                                      allow_custom_downsample_ops=False,\n",
    "                                                    #   multiplicity=8\n",
    "                                                      )\n",
    "\n",
    "# This takes a bit\n",
    "comp_model_prun, stats_prun = ModelCompressor.compress_model(model,\n",
    "                                                   input_shape=input_shape,\n",
    "                                                   eval_callback=eval_callback,\n",
    "                                                   eval_iterations=None,\n",
    "                                                   compress_scheme=CompressionScheme.channel_pruning,\n",
    "                                                   cost_metric=CostMetric.mac,\n",
    "                                                   parameters=channel_pruning_parameters,\n",
    "                                                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WocMph_0o1Iw"
   },
   "source": [
    "Let's look at the two models. Can you see that `comp_model` has some missing channels in the convolutional layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1610297665468,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "HW_7U8RUnwUe",
    "outputId": "6ec98696-025f-4b00-ea47-5bc1467b558f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.967016, Compressed model accuracy: 0.966560\n",
      "Compression ratio for memory=0.972344, mac=0.427639\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.3, compression-ratio: 0.4\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Greedy Eval Dict\n",
      "    Layer: convs.3\n",
      "        Ratio=0.1, Eval score=0.9545707492285121\n",
      "        Ratio=0.2, Eval score=0.9551778216218951\n",
      "        Ratio=0.3, Eval score=0.9546719279607426\n",
      "        Ratio=0.4, Eval score=0.9665604289978247\n",
      "        Ratio=0.5, Eval score=0.9666110183639399\n",
      "        Ratio=0.6, Eval score=0.9663074821672485\n",
      "        Ratio=0.7, Eval score=0.9667121970961704\n",
      "        Ratio=0.8, Eval score=0.9666616077300552\n",
      "        Ratio=0.9, Eval score=0.9669651439267466\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "---------- Original model ----------\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "---------- Compressed model ----------\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Conv2d(1, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(stats_prun)\n",
    "print('-'*10 + ' Original model ' + '-'*10)\n",
    "print(model)\n",
    "print('-'*10 + ' Compressed model ' + '-'*10)\n",
    "print(comp_model_prun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY1XFsmrpE2Y"
   },
   "source": [
    "Now let's see how they compare in terms of speed.\n",
    "Differences probably won't be huge here since the network is quite small to begin with, and only a single layer is pruned, but the point is to see that the compressed one is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55985,
     "status": "ok",
     "timestamp": 1610297835445,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "Kb83uXumopad",
    "outputId": "fe194811-5adc-410a-c261-687638c883e7"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9cf77e6acb7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc_full = eval_callback(model, None, use_cuda=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc_comp = eval_callback(comp_model_prun, None, use_cuda=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d2a14d951043>\u001b[0m in \u001b[0;36meval_callback\u001b[0;34m(model, iterations, use_cuda)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m             raise RuntimeError(\n\u001b[1;32m    195\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "%timeit acc_full = eval_callback(model, None, use_cuda=True)\n",
    "%timeit acc_comp = eval_callback(comp_model_prun, None, use_cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSJJNo9VeXJV"
   },
   "source": [
    "# Noa's try at Spatial SVD with AIMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141074,
     "status": "ok",
     "timestamp": 1610298010127,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "zpzXKE-UeNUd",
    "outputId": "5b264c06-3e9e-4fac-821d-8bc89182c6e6"
   },
   "outputs": [],
   "source": [
    "greedy_params = GreedySelectionParameters(target_comp_ratio=Decimal(0.5))\n",
    "# Do not exclude anything in contrary to channel pruning\n",
    "modules_to_ignore = []\n",
    "auto_params = SpatialSvdParameters.AutoModeParams(greedy_params, modules_to_ignore)\n",
    "input_shape = (1, 1, 28, 28)\n",
    "# Delete all the parameters that don't give an error in channel pruning, but do here\n",
    "spatial_svd_params = SpatialSvdParameters(mode=SpatialSvdParameters.Mode.auto,\n",
    "                                                      params=auto_params)\n",
    "\n",
    "# This takes a bit\n",
    "comp_model_svd, stats_svd = ModelCompressor.compress_model(model,\n",
    "                                                   input_shape=input_shape,\n",
    "                                                   eval_callback=eval_callback,\n",
    "                                                   eval_iterations=None,\n",
    "                                                   compress_scheme=CompressionScheme.spatial_svd,\n",
    "                                                   cost_metric=CostMetric.mac,\n",
    "                                                   parameters=spatial_svd_params,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u_PdEczhLN9"
   },
   "source": [
    "Look at the two models, but now compare it to the Spatial SVD one. You can see that the convolutional layers are split in two. What kind of effect this specificaly has, isn't clear yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1345,
     "status": "ok",
     "timestamp": 1610298036084,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "LzCGSsYnhH2W",
    "outputId": "14ce1e8f-98fb-4bd3-d293-90a715b38075"
   },
   "outputs": [],
   "source": [
    "print(stats_svd)\n",
    "print('-'*10 + ' Original model ' + '-'*10)\n",
    "print(model)\n",
    "print('-'*10 + ' Compressed model ' + '-'*10)\n",
    "print(comp_model_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUhBcSsTiPp9"
   },
   "source": [
    "Compare the speed difference between original model and this new compressed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to have manual selected compression ratio's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57089,
     "status": "ok",
     "timestamp": 1610298249390,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "9WuuBXMBiT7Q",
    "outputId": "d452bd0e-3bb0-4594-b947-08c956a19b11"
   },
   "outputs": [],
   "source": [
    "# model.convs[0] = first conv layer, model.convs[3] = second conv layer\n",
    "manual_params = SpatialSvdParameters.ManualModeParams([ModuleCompRatioPair(model.convs[0], 0.5),\n",
    "                                                           ModuleCompRatioPair(model.convs[3], 0.4)])\n",
    "input_shape = (1, 1, 28, 28)\n",
    "spatial_svd_params = SpatialSvdParameters(mode=SpatialSvdParameters.Mode.manual,\n",
    "                                  params=manual_params)\n",
    "# This takes a bit\n",
    "comp_model_svd, stats_svd = ModelCompressor.compress_model(model,\n",
    "                                                   input_shape=input_shape,\n",
    "                                                   eval_callback=eval_callback,\n",
    "                                                   eval_iterations=None,\n",
    "                                                   compress_scheme=CompressionScheme.spatial_svd,\n",
    "                                                   cost_metric=CostMetric.mac,\n",
    "                                                   parameters=spatial_svd_params,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different ratio's for compression (using greedy search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop to test different compression ratio's on balanced dataset\n",
    "for ratio in np.linspace(0.1,1,18):\n",
    "    print(f\"RATIO = {ratio}\")\n",
    "    greedy_params = GreedySelectionParameters(target_comp_ratio=Decimal(ratio))\n",
    "    modules_to_ignore = []\n",
    "    auto_params = SpatialSvdParameters.AutoModeParams(greedy_params, modules_to_ignore)\n",
    "    input_shape = (1, 1, 28, 28)\n",
    "    # Delete all the parameters that don't give an error in channel pruning, but do here\n",
    "    spatial_svd_params = SpatialSvdParameters(mode=SpatialSvdParameters.Mode.auto,\n",
    "                                                        params=auto_params)\n",
    "\n",
    "    # This takes a bit\n",
    "    comp_model_svd, stats_svd = ModelCompressor.compress_model(model,\n",
    "                                                    input_shape=input_shape,\n",
    "                                                    eval_callback=eval_callback,\n",
    "                                                    eval_iterations=None,\n",
    "                                                    compress_scheme=CompressionScheme.spatial_svd,\n",
    "                                                    cost_metric=CostMetric.mac,\n",
    "                                                    parameters=spatial_svd_params,\n",
    "                                                    )\n",
    "    print(stats_svd)\n",
    "    #%timeit acc_full = eval_callback(model, None, use_cuda=True)\n",
    "    #%timeit acc_comp = eval_callback(comp_model_svd, None, use_cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different ratio's for compression (using manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR IMBALANCE = 2\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "RATIO = 0.9\n",
      "2021-01-13 16:50:46,944 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 3\n",
      "2021-01-13 16:50:46,946 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 19\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 3, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(3, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 19, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(19, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.077550\n",
      "Compression ratio for memory=0.994225, mac=0.840872\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.9\n",
      "    Name:convs.3, compression-ratio: 0.9\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "RATIO = 0.8\n",
      "2021-01-13 16:50:58,566 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 3\n",
      "2021-01-13 16:50:58,568 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 17\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 3, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(3, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 17, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(17, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.090533\n",
      "Compression ratio for memory=0.990646, mac=0.788053\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.8\n",
      "    Name:convs.3, compression-ratio: 0.8\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "RATIO = 0.7\n",
      "2021-01-13 16:51:10,126 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 3\n",
      "2021-01-13 16:51:10,128 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 15\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 3, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(3, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 15, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(15, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.091317\n",
      "Compression ratio for memory=0.987067, mac=0.735235\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.7\n",
      "    Name:convs.3, compression-ratio: 0.7\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "RATIO = 0.6\n",
      "2021-01-13 16:51:21,524 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 2\n",
      "2021-01-13 16:51:21,526 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 13\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(2, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 13, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(13, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.102850\n",
      "Compression ratio for memory=0.982918, mac=0.616537\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.6\n",
      "    Name:convs.3, compression-ratio: 0.6\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "RATIO = 0.5\n",
      "2021-01-13 16:51:33,295 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 2\n",
      "2021-01-13 16:51:33,297 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 10\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(2, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 10, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(10, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.104417\n",
      "Compression ratio for memory=0.977550, mac=0.537309\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.5\n",
      "    Name:convs.3, compression-ratio: 0.5\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "RATIO = 0.4\n",
      "2021-01-13 16:51:45,522 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 16:51:45,525 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 8\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(1, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 8, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(8, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.103933\n",
      "Compression ratio for memory=0.973402, mac=0.418611\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.4\n",
      "    Name:convs.3, compression-ratio: 0.4\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "RATIO = 0.3\n",
      "2021-01-13 16:51:57,483 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 1\n",
      "2021-01-13 16:51:57,486 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 6\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(1, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 6, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(6, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.104417\n",
      "Compression ratio for memory=0.969823, mac=0.365793\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.3\n",
      "    Name:convs.3, compression-ratio: 0.3\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "RATIO = 0.2\n",
      "2021-01-13 16:52:09,095 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 1\n",
      "2021-01-13 16:52:09,098 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 4\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(1, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 4, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(4, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.104417\n",
      "Compression ratio for memory=0.966244, mac=0.312974\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.2\n",
      "    Name:convs.3, compression-ratio: 0.2\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "RATIO = 0.1\n",
      "2021-01-13 16:52:20,719 - Svd - INFO - Spatial SVD splitting layer: convs.0 using rank: 1\n",
      "2021-01-13 16:52:20,722 - Svd - INFO - Spatial SVD splitting layer: convs.3 using rank: 2\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 1, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "      (1): Conv2d(1, 6, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(6, 2, kernel_size=(5, 1), stride=(1, 1), bias=False)\n",
      "      (1): Conv2d(2, 16, kernel_size=(1, 5), stride=(1, 1))\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.084317, Compressed model accuracy: 0.104417\n",
      "Compression ratio for memory=0.962665, mac=0.260156\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.0, compression-ratio: 0.1\n",
      "    Name:convs.3, compression-ratio: 0.1\n",
      "\n",
      "**********************************************************************************************\n",
      "None\n",
      "**********************************************************************************************\n",
      "\n",
      "LINEAR IMBALANCE = 3\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-26896c5b07c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_imbal_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mratios\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# try different ratios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-f9611040cf03>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, opt, train_loader, criterion, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ffa5198a1e67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    140\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 488\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "imbalances = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "ratios = [0.90, 0.80, 0.70, 0.60, 0.50, 0.40, 0.30, 0.20, 0.10]\n",
    "input_shape = (1, 1, 28, 28)\n",
    "\n",
    "\n",
    "for imbal in imbalances: # try different imbalances\n",
    "    print(f\"LINEAR IMBALANCE = {imbal}\")\n",
    "    print(model)\n",
    "    # change imbalance of MNIST dataset\n",
    "    train_imbalance = MNIST_Imbalancer()\n",
    "    train_imbalance.linear_imbalance(imbal)\n",
    "    train = train_imbalance.dataset\n",
    "    train_imbal_loader = DataLoader(train, batch_size=128, num_workers=4, shuffle=True, pin_memory=True)\n",
    "    # use mean squared error loss to not account for the imbalance\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # train model on imbalance with 10 epochs\n",
    "    model = LeNet5().to(device)\n",
    "    for i in range(n_epochs):\n",
    "        train_epoch(model, opt, train_imbal_loader, criterion, device)\n",
    "    \n",
    "    for ratio in ratios: # try different ratios\n",
    "        print(f\"RATIO = {ratio}\")\n",
    "        # apply same ratio on both conv layers\n",
    "        manual_params = SpatialSvdParameters.ManualModeParams([ModuleCompRatioPair(model.convs[0], ratio),\n",
    "                                                               ModuleCompRatioPair(model.convs[3], ratio)]\n",
    "                                                             )\n",
    "        spatial_svd_params = SpatialSvdParameters(mode=SpatialSvdParameters.Mode.manual,\n",
    "                                                  params=manual_params\n",
    "                                                 )\n",
    "        comp_model_svd, stats_svd = ModelCompressor.compress_model(model,\n",
    "                                                           input_shape=input_shape,\n",
    "                                                           eval_callback=eval_callback,\n",
    "                                                           eval_iterations=None,\n",
    "                                                           compress_scheme=CompressionScheme.spatial_svd,\n",
    "                                                           cost_metric=CostMetric.mac,\n",
    "                                                           parameters=spatial_svd_params\n",
    "                                                           )\n",
    "        print(comp_model_svd)\n",
    "        print(stats_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "our_aimet_ready.ipynb",
   "provenance": [
    {
     "file_id": "1IOgTNChF3Gmc_zEfY-3DpxcbeSCDxaUd",
     "timestamp": 1609924035770
    },
    {
     "file_id": "1_U4qdGKGRxnjczFdBOrN1O6AbisFxFTc",
     "timestamp": 1607621041273
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "012c3c9adc694baba6feb904d60f9201": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b5ccd5354b040b58f645351f4d950f3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b0a62dffc4341069d860e1e7c09a85c",
      "value": 1
     }
    },
    "0b0a62dffc4341069d860e1e7c09a85c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "154e97cc69984a0ab11ced26576ebbd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab3eb90c487746e8b65e62d7757c0e78",
      "placeholder": "",
      "style": "IPY_MODEL_c41636ad0caf4621870bd12e89518604",
      "value": " 32768/? [00:00&lt;00:00, 96064.27it/s]"
     }
    },
    "15b6b7d213c94345b6dedf59e8e065dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15c6e5135ed849bc89dec5bf0a38363b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab99643118ed466298c1ea12d4ac53d9",
      "placeholder": "",
      "style": "IPY_MODEL_c9dc0f6d46b74558bcb4962e3f1fb489",
      "value": " 0/? [00:00&lt;?, ?it/s]"
     }
    },
    "1af67330e5d947718a1e80e982898052": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2328a7b009d44c5e9eacff2a755cbebb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_46c25a22208a49eda4e6eab32d4eaf2f",
       "IPY_MODEL_d4ff65d7c61d4bf9b73c03e29c28a8ad"
      ],
      "layout": "IPY_MODEL_ed9207ce942047e9ae6072508f523741"
     }
    },
    "2a5ab588d2f24b0ab3ec8ce388f5127f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3b5ccd5354b040b58f645351f4d950f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46c25a22208a49eda4e6eab32d4eaf2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5f8150fe3124fc88dc714b404006f34",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a5ab588d2f24b0ab3ec8ce388f5127f",
      "value": 1
     }
    },
    "4b07de9e194f4439baeae01257e4ba52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7989862673284a799c9d51e431373247",
       "IPY_MODEL_154e97cc69984a0ab11ced26576ebbd7"
      ],
      "layout": "IPY_MODEL_bd10cc47f292451d922d8980f5e93ddf"
     }
    },
    "543c9bdec83f486eaa8b9b2b58ef211a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5503b3b0dbb548589aaa0279b976a414": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6126bcc00c54407688d7d3a081fa43c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1af67330e5d947718a1e80e982898052",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_657579abb9c4488c8f1a810a679dfda9",
      "value": 0
     }
    },
    "657579abb9c4488c8f1a810a679dfda9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7989862673284a799c9d51e431373247": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efa8ccf950004549a00250f6332bc16b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5503b3b0dbb548589aaa0279b976a414",
      "value": 1
     }
    },
    "8ce0005792b74005a30cb426b5c8077a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fffcd6e9d7a449fb4bfccedc6cb70a1",
      "placeholder": "",
      "style": "IPY_MODEL_543c9bdec83f486eaa8b9b2b58ef211a",
      "value": " 1654784/? [00:00&lt;00:00, 6376640.54it/s]"
     }
    },
    "8da6d53cc36743e6bd5058b8840e0c41": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fffcd6e9d7a449fb4bfccedc6cb70a1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5f8150fe3124fc88dc714b404006f34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab3eb90c487746e8b65e62d7757c0e78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab99643118ed466298c1ea12d4ac53d9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac82f07b477b4502b21ebaa8a539c4a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af3a484183a24008a6d38960f73ffb3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_012c3c9adc694baba6feb904d60f9201",
       "IPY_MODEL_8ce0005792b74005a30cb426b5c8077a"
      ],
      "layout": "IPY_MODEL_ac82f07b477b4502b21ebaa8a539c4a3"
     }
    },
    "bd10cc47f292451d922d8980f5e93ddf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c41636ad0caf4621870bd12e89518604": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9dc0f6d46b74558bcb4962e3f1fb489": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4ff65d7c61d4bf9b73c03e29c28a8ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6769ba59c21406e8c259661bdbf68f1",
      "placeholder": "",
      "style": "IPY_MODEL_8da6d53cc36743e6bd5058b8840e0c41",
      "value": " 9920512/? [00:20&lt;00:00, 27818522.57it/s]"
     }
    },
    "df98becb207b42f2bbeaf23708e5be02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6126bcc00c54407688d7d3a081fa43c0",
       "IPY_MODEL_15c6e5135ed849bc89dec5bf0a38363b"
      ],
      "layout": "IPY_MODEL_15b6b7d213c94345b6dedf59e8e065dc"
     }
    },
    "ed9207ce942047e9ae6072508f523741": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efa8ccf950004549a00250f6332bc16b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6769ba59c21406e8c259661bdbf68f1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
