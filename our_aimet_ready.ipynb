{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z5YVNCz9mLL"
   },
   "source": [
    "# AIMET dependencies install & build\n",
    "The following group of cells installs the AIMET library for you. For more details, please see [this link](https://github.com/quic/aimet/blob/develop/packaging/google_colab_install.md).\n",
    "\n",
    "You can clone this notebook and use it in your own project. Make sure that before running these cells, you connect to a hosted environment with a GPU accelerator. (Runtime -> Change runtime -> Hardware Accelerator(GPU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Hz_C0-x_hbw"
   },
   "source": [
    "## Installing dependencies\n",
    "May prompt you.\n",
    "# Ignore first 4 code cells when running in docker!!!\n",
    "## Start from \"Train a model on MNIST data\" :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 832825,
     "status": "ok",
     "timestamp": 1610296563283,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "xCMVKw4N9lYv",
    "outputId": "979c56ce-080f-4e3c-e056-9b77d6ad24e6"
   },
   "source": [
    "!pip3 uninstall --yes protobuf\n",
    "!pip3 uninstall --yes tensorflow\n",
    "!apt-get update\n",
    "!apt-get install python3.6\n",
    "!apt-get install python3-dev\n",
    "!apt-get install python3-pip\n",
    "!apt-get install liblapacke liblapacke-dev\n",
    "!apt-get install wget\n",
    "!pip3 install numpy==1.16.4\n",
    "!apt-get install libgtest-dev build-essential cmake\n",
    "!pip3 --no-cache-dir install opencv-python==4.1.0.25\n",
    "!pip3 --no-cache-dir install pillow==6.2.1\n",
    "!pip3 install pytorch-ignite==0.1.0\n",
    "!wget -q https://github.com/Itseez/opencv/archive/3.1.0.tar.gz -O /tmp/3.1.0.tar.gz > /dev/null\n",
    "!tar -C /tmp -xvf /tmp/3.1.0.tar.gz > /dev/null\n",
    "%cd /tmp/opencv-3.1.0\n",
    "%mkdir release\n",
    "%cd release\n",
    "!cmake -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DBUILD_SHARED_LIBS=OFF -DCMAKE_BUILD_TYPE=release -DWITH_FFMPEG=OFF -DBUILD_TESTS=OFF -DWITH_CUDA=OFF -DBUILD_PERF_TESTS=OFF -DWITH_IPP=OFF -DENABLE_PRECOMPILED_HEADERS=OFF .. > /dev/null\n",
    "!make -j16 > /dev/null\n",
    "!make -j16 install > /dev/null\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
    "!apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
    "!dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
    "!apt-get update\n",
    "!wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
    "!apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
    "!apt-get update\n",
    "!apt install cuda-cublas-10-0 cuda-cufft-10-0 cuda-curand-10-0 cuda-cusolver-10-0\n",
    "!apt-get update && apt install cuda-cusparse-10-0 libcudnn7=7.6.2.24-1+cuda10.0 libnccl2=2.4.8-1+cuda10.0  cuda-command-line-tools-10.0\n",
    "!pip3 install scipy==1.1.0\n",
    "!pip3 install protobuf==3.7.1\n",
    "!pip3 install scikit-learn==0.19.1\n",
    "!pip3 install tb-nightly==1.14.0a20190517\n",
    "!pip3 install tensorboardX==1.7\n",
    "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.5.0%2Bcu100-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install tensorflow-gpu==1.15.0\n",
    "!pip3 install future==0.17.1\n",
    "!pip3 install tensorboard==1.14\n",
    "!pip3 install bokeh==1.2.0\n",
    "!pip3 install pandas==0.22.0\n",
    "!pip3 install holoviews==1.12.7\n",
    "!pip3 install --no-deps bokeh==1.2.0 hvplot==0.4.0\n",
    "!pip3 install jsonschema==3.1.1\n",
    "!pip3 install osqp onnx\n",
    "\n",
    "!ln -s /usr/local/cuda-10.0 /usr/local/cuda\n",
    "!apt-get update && apt-get install -y libjpeg8-dev\n",
    "!ln -s /usr/lib/x86_64-linux-gnu/libjpeg.so /usr/lib\n",
    "\n",
    "!apt install zlib1g-dev\n",
    "\n",
    "!pip3 uninstall --yes Pillow && pip3 install Pillow-SIMD==6.0.0.post0\n",
    "!pip3 uninstall --yes pytest\n",
    "!pip3 install pytest\n",
    "!pip3 install setuptools==41.0.1\n",
    "!pip3 install keras==2.2.4\n",
    "\n",
    "%rm -rf /usr/local/bin/python\n",
    "!ln -s /usr/bin/python3 /usr/local/bin/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiiZL_wi_oeP"
   },
   "source": [
    "After installing the dependencies, you must restart the environment before proceeding. (Runtime -> Restart Runtime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdF9TK03CKiW"
   },
   "source": [
    "## AIMET build and installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115891,
     "status": "ok",
     "timestamp": 1610296813311,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "1nR1MrsI-EW1",
    "outputId": "03b22e94-4bbf-499e-c364-df5b131f2975",
    "scrolled": true
   },
   "source": [
    "%cd /content/\n",
    "!rm -rf aimet_code\n",
    "!mkdir aimet_code\n",
    "%cd aimet_code\n",
    "!git clone https://github.com/quic/aimet.git\n",
    "%cd aimet\n",
    "%mkdir -p ./ThirdParty/googletest\n",
    "%pushd ./ThirdParty/googletest\n",
    "!git clone https://github.com/google/googletest.git -b release-1.8.0 googletest-release-1.8.0\n",
    "%popd\n",
    "%cd /content/aimet_code\n",
    "%mkdir build\n",
    "%cd build\n",
    "!cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON ../aimet\n",
    "!make -j 8\n",
    "!make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6qARmUzAZj4"
   },
   "source": [
    "## Setting up `PYTHONPATH` and `LD_LIBRARY_PATH`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INEc0N5xAbuz"
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r'/content/aimet_code/build/staging/universal/lib/python')\n",
    "sys.path.append(r'/content/aimet_code/build/staging/universal/lib/x86_64-linux-gnu')\n",
    "sys.path.append(r'/usr/local/lib/python3.6/dist-packages')\n",
    "sys.path.append(r'/content/aimet_code/build/artifacts')\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['LD_LIBRARY_PATH']+= \":/content/aimet_code/build/artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DiI-1hvAhd3"
   },
   "source": [
    "## Run unit tests\n",
    "If the installation went smoothly, all tests should pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 438596,
     "status": "ok",
     "timestamp": 1610297270113,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "WhO4FvxHAijc",
    "outputId": "fdf0203b-f52d-4e6f-f89f-722b155fcabf"
   },
   "source": [
    "%cd /content/aimet_code/build/\n",
    "!ctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZJ8R7fCRx_f"
   },
   "source": [
    "# Train a model on MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXn0f41ueDFN"
   },
   "source": [
    "Set random seed for reprodubicility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1610306425865,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "Y-GPWZa2eFS1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_grayscale(tensor):\n",
    "    plt.imshow(tensor, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_zQ5VelR30T"
   },
   "source": [
    "Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1610306428470,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "z9-dFvJhRxsf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5(torch.nn.Module):          \n",
    "\n",
    "    def __init__(self):     \n",
    "        super(LeNet5, self).__init__()\n",
    "        self.convs = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2),\n",
    "                                    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding=0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2)\n",
    "                                  )\n",
    "\n",
    "        self.linears = nn.Sequential(nn.Linear(16*5*5, 120),\n",
    "                                     nn.Linear(120, 84),\n",
    "                                     nn.Linear(84, 10)\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.linears(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for creating imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def keep_selection(dataset, target, selection):\n",
    "    ''' create imbalance in single class of a dataset '''\n",
    "    \n",
    "    # get indices of imgs of target number and remove selection of the indices\n",
    "    target_mask = dataset.targets == target\n",
    "    selection_idx = target_mask.nonzero()[round(len(target_mask.nonzero())*selection):]\n",
    "    \n",
    "    # make mask wich selects all data except for indices in selection\n",
    "    selection_mask = np.ones(len(dataset.data), dtype=bool)\n",
    "    selection_mask[selection_idx] = False\n",
    "    \n",
    "    # apply mask to remove the selected data\n",
    "    dataset.data = dataset.data[selection_mask]\n",
    "    dataset.targets = dataset.targets[selection_mask]\n",
    "\n",
    "\n",
    "def keep_selections(dataset, selection_dict):\n",
    "    ''' create imbalance in dataset according to selection dict '''\n",
    "    \n",
    "    # generate random ordered indeces for dataset\n",
    "    datapoints = dataset.data.shape[0]\n",
    "    rand_idx = torch.randperm(datapoints)\n",
    "    \n",
    "    # shuffle data and targets in the same way\n",
    "    dataset.data = dataset.data[rand_idx]\n",
    "    dataset.targets = dataset.targets[rand_idx]\n",
    "    \n",
    "    # throw away a part of the data for each class\n",
    "    for class_number, selection in selection_dict.items():\n",
    "        keep_selection(dataset, class_number, selection)\n",
    "        \n",
    "def linear_imbalance(dataset, ordered_classes, min_examples):\n",
    "    ''' create selection dict with linear imbalance '''\n",
    "    \n",
    "    n_steps = len(ordered_classes) - 1\n",
    "    linear_step = (1.0 - min_examples) / n_steps\n",
    "    selection_dict = dict()\n",
    "    \n",
    "    # interpolate the classes between the minimum and maximum linearly\n",
    "    for i, data_class in enumerate(ordered_classes):\n",
    "        selection_dict[data_class] = min_examples + (i * linear_step)\n",
    "    \n",
    "    # make the selection from the dataset\n",
    "    keep_selections(dataset, selection_dict)\n",
    "    \n",
    "    return selection_dict\n",
    "\n",
    "def step_imbalance(dataset, ordered_classes, min_examples, step):\n",
    "    ''' create selection dict with step imbalance '''\n",
    "    \n",
    "    n_classes = len(ordered_classes)\n",
    "    selection_dict = dict()\n",
    "    step_index = math.floor(step * n_classes)\n",
    "    \n",
    "    for i, data_class in enumerate(ordered_classes):\n",
    "        if i < step_index:\n",
    "            selection_dict[data_class] = min_examples\n",
    "        else:\n",
    "            selection_dict[data_class] = 1.0\n",
    "    \n",
    "    # make the selection from the dataset\n",
    "    keep_selections(dataset, selection_dict)\n",
    "    \n",
    "    return selection_dict\n",
    "\n",
    "\n",
    "def long_tailed_imbalance(dataset, ordered_classes, mu):\n",
    "    ''' create selection dict with long-tailed imbalance'''\n",
    "    selection_dict = dict()\n",
    "    \n",
    "    # set selection for each class according to long-tailed function, mu is in (0,1)\n",
    "    for i, data_class in enumerate(ordered_classes):\n",
    "        selection_dict[data_class] = mu**i\n",
    "    \n",
    "    # make the selection from the dataset\n",
    "    keep_selections(dataset, selection_dict)\n",
    "    \n",
    "    return selection_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484,
     "referenced_widgets": [
      "2328a7b009d44c5e9eacff2a755cbebb",
      "ed9207ce942047e9ae6072508f523741",
      "46c25a22208a49eda4e6eab32d4eaf2f",
      "d4ff65d7c61d4bf9b73c03e29c28a8ad",
      "2a5ab588d2f24b0ab3ec8ce388f5127f",
      "a5f8150fe3124fc88dc714b404006f34",
      "8da6d53cc36743e6bd5058b8840e0c41",
      "f6769ba59c21406e8c259661bdbf68f1",
      "4b07de9e194f4439baeae01257e4ba52",
      "bd10cc47f292451d922d8980f5e93ddf",
      "7989862673284a799c9d51e431373247",
      "154e97cc69984a0ab11ced26576ebbd7",
      "5503b3b0dbb548589aaa0279b976a414",
      "efa8ccf950004549a00250f6332bc16b",
      "c41636ad0caf4621870bd12e89518604",
      "ab3eb90c487746e8b65e62d7757c0e78",
      "af3a484183a24008a6d38960f73ffb3e",
      "ac82f07b477b4502b21ebaa8a539c4a3",
      "012c3c9adc694baba6feb904d60f9201",
      "8ce0005792b74005a30cb426b5c8077a",
      "0b0a62dffc4341069d860e1e7c09a85c",
      "3b5ccd5354b040b58f645351f4d950f3",
      "543c9bdec83f486eaa8b9b2b58ef211a",
      "9fffcd6e9d7a449fb4bfccedc6cb70a1",
      "df98becb207b42f2bbeaf23708e5be02",
      "15b6b7d213c94345b6dedf59e8e065dc",
      "6126bcc00c54407688d7d3a081fa43c0",
      "15c6e5135ed849bc89dec5bf0a38363b",
      "657579abb9c4488c8f1a810a679dfda9",
      "1af67330e5d947718a1e80e982898052",
      "c9dc0f6d46b74558bcb4962e3f1fb489",
      "ab99643118ed466298c1ea12d4ac53d9"
     ]
    },
    "executionInfo": {
     "elapsed": 2453,
     "status": "ok",
     "timestamp": 1610306438151,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "kH5o32FLVJ2D",
    "outputId": "49acbac6-c0ea-4f76-b479-d7bf33ecacfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.1307,), std=(0.3081,))\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Convert imgs to tensor and normalize by mean and stddev of the training set\n",
    "transformImg = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "print(transformImg)\n",
    "\n",
    "train = MNIST(root='./data', train=True, download=True, transform=transformImg)\n",
    "test = MNIST(root='./data', train=False, download=True, transform=transformImg)\n",
    "\n",
    "MNIST_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "linear_dict = linear_imbalance(train, MNIST_classes, 0.5)\n",
    "\n",
    "plt.bar(linear_dict.keys(), linear_dict.values(), color='g')\n",
    "plt.title('Linear Imbalance')\n",
    "plt.show()\n",
    "\n",
    "step_dict = step_imbalance(train, MNIST_classes, 0.3, 0.5)\n",
    "\n",
    "plt.bar(step_dict.keys(), step_dict.values(), color='r')\n",
    "plt.title('Step Imbalance')\n",
    "plt.show()\n",
    "\n",
    "long_tailed_dict = long_tailed_imbalance(train, MNIST_classes, 0.7)\n",
    "\n",
    "plt.bar(long_tailed_dict.keys(), long_tailed_dict.values(), color='b')\n",
    "plt.title('Long_tailed Imbalance')\n",
    "plt.show()\n",
    "\n",
    "MNIST_selection_dict = {0: .1, 1: .2, 2: .3, 3: .4, 4: .5, 5: .6, 6: .7, 7: .8, 8: .9, 9: 1.}\n",
    "\n",
    "# applies unbalance to dataset\n",
    "keep_selections(train, long_tailed_dict)\n",
    "\n",
    "# Define train/test loaders\n",
    "train_loader = DataLoader(train, batch_size=128, num_workers=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(train, batch_size=1024, num_workers=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484,
     "referenced_widgets": [
      "2328a7b009d44c5e9eacff2a755cbebb",
      "ed9207ce942047e9ae6072508f523741",
      "46c25a22208a49eda4e6eab32d4eaf2f",
      "d4ff65d7c61d4bf9b73c03e29c28a8ad",
      "2a5ab588d2f24b0ab3ec8ce388f5127f",
      "a5f8150fe3124fc88dc714b404006f34",
      "8da6d53cc36743e6bd5058b8840e0c41",
      "f6769ba59c21406e8c259661bdbf68f1",
      "4b07de9e194f4439baeae01257e4ba52",
      "bd10cc47f292451d922d8980f5e93ddf",
      "7989862673284a799c9d51e431373247",
      "154e97cc69984a0ab11ced26576ebbd7",
      "5503b3b0dbb548589aaa0279b976a414",
      "efa8ccf950004549a00250f6332bc16b",
      "c41636ad0caf4621870bd12e89518604",
      "ab3eb90c487746e8b65e62d7757c0e78",
      "af3a484183a24008a6d38960f73ffb3e",
      "ac82f07b477b4502b21ebaa8a539c4a3",
      "012c3c9adc694baba6feb904d60f9201",
      "8ce0005792b74005a30cb426b5c8077a",
      "0b0a62dffc4341069d860e1e7c09a85c",
      "3b5ccd5354b040b58f645351f4d950f3",
      "543c9bdec83f486eaa8b9b2b58ef211a",
      "9fffcd6e9d7a449fb4bfccedc6cb70a1",
      "df98becb207b42f2bbeaf23708e5be02",
      "15b6b7d213c94345b6dedf59e8e065dc",
      "6126bcc00c54407688d7d3a081fa43c0",
      "15c6e5135ed849bc89dec5bf0a38363b",
      "657579abb9c4488c8f1a810a679dfda9",
      "1af67330e5d947718a1e80e982898052",
      "c9dc0f6d46b74558bcb4962e3f1fb489",
      "ab99643118ed466298c1ea12d4ac53d9"
     ]
    },
    "executionInfo": {
     "elapsed": 2453,
     "status": "ok",
     "timestamp": 1610306438151,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "kH5o32FLVJ2D",
    "outputId": "49acbac6-c0ea-4f76-b479-d7bf33ecacfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.1307,), std=(0.3081,))\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear_imbalance() missing 1 required positional argument: 'min_examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1642b366218d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mMNIST_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mlinear_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_imbalance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNIST_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear_imbalance() missing 1 required positional argument: 'min_examples'"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Convert imgs to tensor and normalize by mean and stddev of the training set\n",
    "transformImg = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "print(transformImg)\n",
    "\n",
    "train = MNIST(root='./data', train=True, download=True, transform=transformImg)\n",
    "test = MNIST(root='./data', train=False, download=True, transform=transformImg)\n",
    "\n",
    "def keep_selection(dataset, target, selection):\n",
    "    ''' create imbalance in single class of a dataset '''\n",
    "    \n",
    "    # get indices of imgs of target number and remove selection of the indices\n",
    "    target_mask = dataset.targets == target\n",
    "    selection_idx = target_mask.nonzero()[round(len(target_mask.nonzero())*selection):]\n",
    "    \n",
    "    # make mask wich selects all data except for indices in selection\n",
    "    selection_mask = np.ones(len(dataset.data), dtype=bool)\n",
    "    selection_mask[selection_idx] = False\n",
    "    \n",
    "    # apply mask to remove the selected data\n",
    "    dataset.data = dataset.data[selection_mask]\n",
    "    dataset.targets = dataset.targets[selection_mask]\n",
    "\n",
    "\n",
    "def keep_selections(dataset, selection_dict):\n",
    "    ''' create imbalance in dataset according to selection dict '''\n",
    "    \n",
    "    # generate random ordered indeces for dataset\n",
    "    datapoints = dataset.data.shape[0]\n",
    "    rand_idx = torch.randperm(datapoints)\n",
    "    \n",
    "    # shuffle data and targets in the same way\n",
    "    dataset.data = dataset.data[rand_idx]\n",
    "    dataset.targets = dataset.targets[rand_idx]\n",
    "    \n",
    "    # throw away a part of the data for each class\n",
    "    for class_number, selection in selection_dict.items():\n",
    "        keep_selection(dataset, class_number, selection)\n",
    "        \n",
    "def linear_imbalance(dataset, ordered_classes, min_examples):\n",
    "    ''' create selection dict with linear imbalance '''\n",
    "    \n",
    "    n_steps = len(ordered_classes) - 1\n",
    "    linear_step = (1.0 - min_examples) / n_steps\n",
    "    selection_dict = dict()\n",
    "    \n",
    "    # interpolate the classes between the minimum and maximum linearly\n",
    "    for i, data_class in enumerate(ordered_classes):\n",
    "        selection_dict[data_class] = min_examples + (i * linear_step)\n",
    "    \n",
    "    # make the selection from the dataset\n",
    "    return keep_selections(dataset, selection_dict)\n",
    "\n",
    "def step_imbalance(ordered_classes, min_examples, step):\n",
    "    ''' create selection dict with step imbalance '''\n",
    "    \n",
    "    n_classes = len(ordered_classes)\n",
    "    selection_dict = dict()\n",
    "    step_index = floor(mean * n_classes)\n",
    "    \n",
    "    for i, data_class in enumerate(ordered_classes):\n",
    "        if i <= step_index:\n",
    "            selection_dict[data_class] = min_examples\n",
    "        else:\n",
    "            selection_dict[data_class] = 1.0\n",
    "    \n",
    "    # make the selection from the dataset\n",
    "    return keep_selections(dataset, selection_dict)\n",
    "\n",
    "def long_tailed_imbalance(ordered_classes, mu):\n",
    "    ''' create selection dict with long-tailed imbalance'''\n",
    "    selection_dict = dict()\n",
    "    \n",
    "    # set selection for each class according to long-tailed function, mu is in (0,1)\n",
    "    for i, data_class in enumerate(ordered_classes):\n",
    "        selection_dict[data_class] = mu**i\n",
    "    \n",
    "    return selection_dict\n",
    "\n",
    "\n",
    "MNIST_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "linear_dict = linear_imbalance(MNIST_classes, 0.5)\n",
    "\n",
    "plt.bar(linear_dict.keys(), linear_dict.values(), color='g')\n",
    "plt.title('Linear Imbalance')\n",
    "plt.show()\n",
    "\n",
    "long_tailed_dict = long_tailed_imbalance(MNIST_classes, 0.7)\n",
    "\n",
    "plt.bar(long_tailed_dict.keys(), long_tailed_dict.values(), color='b')\n",
    "plt.title('Long_tailed Imbalance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "MNIST_selection_dict = {0: .1, 1: .2, 2: .3, 3: .4, 4: .5, 5: .6, 6: .7, 7: .8, 8: .9, 9: 1.}\n",
    "\n",
    "# applies unbalance to dataset\n",
    "keep_selections(train, long_tailed_dict)\n",
    "\n",
    "# Define train/test loaders\n",
    "train_loader = DataLoader(train, batch_size=128, num_workers=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(train, batch_size=1024, num_workers=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iRixegDe7Ot"
   },
   "source": [
    "Define functions for conducting the training and testing epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1610306455425,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "BgDcuZVhe9C4"
   },
   "outputs": [],
   "source": [
    "def accuracy(out, y):\n",
    "    preds = out.argmax(dim=1, keepdim=True).squeeze()\n",
    "    correct = preds.eq(y).sum().item()\n",
    "    return correct\n",
    "\n",
    "def train_epoch(model, opt, train_loader, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    n_samples = 0\n",
    "    for x,y in train_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model.forward(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        epoch_acc += accuracy(out, y)\n",
    "        epoch_loss += loss.item()\n",
    "        n_samples += x.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    epoch_acc = epoch_acc / n_samples\n",
    "    epoch_loss = epoch_loss / n_samples\n",
    "\n",
    "    return epoch_acc, epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "def test_epoch(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model.forward(x)\n",
    "            loss = criterion(out, y)\n",
    "\n",
    "            epoch_acc += accuracy(out, y)\n",
    "            epoch_loss += loss.item()\n",
    "            n_samples += x.size(0)\n",
    "\n",
    "    epoch_acc = epoch_acc / n_samples\n",
    "    epoch_loss = epoch_loss / n_samples\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w97dpw3dyde"
   },
   "source": [
    "Instantiate required objects and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D28i0tp6VqTZ"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# to use for weighted loss functions, values should be according to imbalance\n",
    "weights = torch.FloatTensor([1,1,1,1,1,1,2,1,1,2])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epochs = 10\n",
    "writer = SummaryWriter()\n",
    "model = LeNet5().to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Train+test, log to tensorboard\n",
    "# It's recommended to also print all the scalar values\n",
    "for i in range(n_epochs):\n",
    "    train_acc, train_loss = train_epoch(model, opt, train_loader, criterion, device)\n",
    "    test_acc, test_loss = test_epoch(model, test_loader, criterion, device)\n",
    "    writer.add_scalar('train/acc', train_acc, i+1)\n",
    "    writer.add_scalar('train/loss', train_loss, i+1)\n",
    "    writer.add_scalar('test/acc', test_acc, i+1)\n",
    "    writer.add_scalar('test/loss', test_loss, i+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW_XJaFxlWFk"
   },
   "source": [
    "Let's check out how the model did with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 5154,
     "status": "ok",
     "timestamp": 1610307800543,
     "user": {
      "displayName": "Jelle",
      "photoUrl": "",
      "userId": "13664908576423573267"
     },
     "user_tz": -60
    },
    "id": "A3fkVkHYfMi9",
    "outputId": "9088161d-a432-4182-ff53-81f7fbd8454a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard-plugin-wit\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)\n",
      "\u001b[K    100% || 788kB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit\n",
      "Successfully installed tensorboard-plugin-wit-1.7.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-39db32bbd2c69532\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-39db32bbd2c69532\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "!pip3 install tensorboard-plugin-wit\n",
    "%tensorboard --logdir ./runs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p7dzY73mfBQ"
   },
   "source": [
    "# Do channel pruning with AIMET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFFOWmSmmh0H"
   },
   "source": [
    "Import necessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4679,
     "status": "ok",
     "timestamp": 1610297563542,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "mE-CvbnmmhMl",
    "outputId": "0fae64dd-1031-4c55-c2d1-63293f2e3ed6"
   },
   "outputs": [],
   "source": [
    "from aimet_common.defs import CostMetric, CompressionScheme, GreedySelectionParameters\n",
    "from aimet_torch.defs import ChannelPruningParameters\n",
    "from aimet_torch.compress import ModelCompressor\n",
    "from aimet_torch.onnx_utils import OnnxSaver\n",
    "from decimal import Decimal\n",
    "\n",
    "# Model compressor needs an evaluation function with this specific signature\n",
    "def eval_callback(model, iterations, use_cuda=True):\n",
    "    model.eval()\n",
    "    epoch_acc = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for idx,(x,y) in enumerate(test_loader):\n",
    "            if use_cuda:\n",
    "                x, y = x.to('cuda:0'), y.to('cuda:0')\n",
    "\n",
    "            out = model.forward(x)\n",
    "            epoch_acc += accuracy(out, y)\n",
    "            n_samples += x.size(0)\n",
    "\n",
    "            if iterations is not None:\n",
    "                if idx == iterations:\n",
    "                    break\n",
    "        epoch_acc = epoch_acc / n_samples\n",
    "\n",
    "    return epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCSBR_ByrT4z"
   },
   "source": [
    "Do the actual pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83306,
     "status": "ok",
     "timestamp": 1610297656200,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "N75lsRaAmna1",
    "outputId": "c3ff1dcd-ae7c-4b05-888b-01ff888a993f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:47,995 - CompRatioSelect - INFO - Analyzing compression ratio: 0.1 =====================>\n",
      "2021-01-13 10:15:48,027 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:48,225 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:48,453 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.100000 ==> eval_score=0.818600\n",
      "2021-01-13 10:15:48,454 - CompRatioSelect - INFO - Analyzing compression ratio: 0.2 =====================>\n",
      "2021-01-13 10:15:48,474 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:48,642 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:48,931 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.200000 ==> eval_score=0.819061\n",
      "2021-01-13 10:15:48,932 - CompRatioSelect - INFO - Analyzing compression ratio: 0.3 =====================>\n",
      "2021-01-13 10:15:48,959 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:49,144 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:49,392 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.300000 ==> eval_score=0.817680\n",
      "2021-01-13 10:15:49,393 - CompRatioSelect - INFO - Analyzing compression ratio: 0.4 =====================>\n",
      "2021-01-13 10:15:49,414 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:49,597 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:49,838 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.400000 ==> eval_score=0.846225\n",
      "2021-01-13 10:15:49,839 - CompRatioSelect - INFO - Analyzing compression ratio: 0.5 =====================>\n",
      "2021-01-13 10:15:49,859 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:50,037 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:50,303 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.500000 ==> eval_score=0.845764\n",
      "2021-01-13 10:15:50,304 - CompRatioSelect - INFO - Analyzing compression ratio: 0.6 =====================>\n",
      "2021-01-13 10:15:50,341 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:50,500 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:50,765 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.600000 ==> eval_score=0.847145\n",
      "2021-01-13 10:15:50,766 - CompRatioSelect - INFO - Analyzing compression ratio: 0.7 =====================>\n",
      "2021-01-13 10:15:50,786 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:50,983 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:51,233 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.700000 ==> eval_score=0.844843\n",
      "2021-01-13 10:15:51,234 - CompRatioSelect - INFO - Analyzing compression ratio: 0.8 =====================>\n",
      "2021-01-13 10:15:51,258 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:51,409 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:51,705 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.800000 ==> eval_score=0.844843\n",
      "2021-01-13 10:15:51,706 - CompRatioSelect - INFO - Analyzing compression ratio: 0.9 =====================>\n",
      "2021-01-13 10:15:51,736 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:51,928 - ChannelPruning - INFO - finished linear regression fit \n",
      "2021-01-13 10:15:52,202 - CompRatioSelect - INFO - Layer convs.3, comp_ratio 0.900000 ==> eval_score=0.844843\n",
      "2021-01-13 10:15:52,204 - CompRatioSelect - INFO - Greedy selection: Saved eval dict to ./data/greedy_selection_eval_scores_dict.pkl\n",
      "2021-01-13 10:15:52,207 - CompRatioSelect - INFO - Greedy selection: overall_min_score=0.817680, overall_max_score=0.847145\n",
      "2021-01-13 10:15:52,209 - CompRatioSelect - INFO - Greedy selection: Original model cost=(Cost: memory=61470, mac=416520)\n",
      "2021-01-13 10:15:52,242 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 10:15:52,292 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 10:15:52,338 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 10:15:52,381 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 10:15:52,426 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/build/staging/universal/lib/python/aimet_torch/winnow/winnow_utils.py:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result = torch.tensor(tensor)  # pylint: disable=not-callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:15:52,484 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 10:15:52,509 - CompRatioSelect - INFO - Greedy selection: final choice - comp_ratio=0.427639, score=0.844843\n",
      "2021-01-13 10:15:52,529 - ConnectedGraph - WARNING - Ops with missing modules: ['flatten_6']\n",
      "This can be due to several reasons:\n",
      "1. There is no mapping for the op in ConnectedGraph.op_type_map. Add a mapping for ConnectedGraph to recognize and be able to map the op.\n",
      "2. The op is defined as a functional in the forward function, instead of as a class module. Redefine the op as a class module if possible. Else, check 3.\n",
      "3. This op is one that cannot be defined as a class module, but has not been added to ConnectedGraph.functional_ops. Add to continue.\n",
      "2021-01-13 10:15:52,715 - ChannelPruning - INFO - finished linear regression fit \n"
     ]
    }
   ],
   "source": [
    "greedy_params = GreedySelectionParameters(target_comp_ratio=Decimal(0.5))\n",
    "# Exclude first layer from pruning\n",
    "modules_to_ignore = [model.convs[0]]\n",
    "auto_params = ChannelPruningParameters.AutoModeParams(greedy_params, modules_to_ignore)\n",
    "input_shape = (1, 1, 28, 28)\n",
    "channel_pruning_parameters = ChannelPruningParameters(mode=ChannelPruningParameters.Mode.auto,\n",
    "                                                      params=auto_params,\n",
    "                                                      data_loader=train_loader,\n",
    "                                                      num_reconstruction_samples=1024,\n",
    "                                                      allow_custom_downsample_ops=False,\n",
    "                                                    #   multiplicity=8\n",
    "                                                      )\n",
    "\n",
    "# This takes a bit\n",
    "comp_model_prun, stats_prun = ModelCompressor.compress_model(model,\n",
    "                                                   input_shape=input_shape,\n",
    "                                                   eval_callback=eval_callback,\n",
    "                                                   eval_iterations=None,\n",
    "                                                   compress_scheme=CompressionScheme.channel_pruning,\n",
    "                                                   cost_metric=CostMetric.mac,\n",
    "                                                   parameters=channel_pruning_parameters,\n",
    "                                                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WocMph_0o1Iw"
   },
   "source": [
    "Let's look at the two models. Can you see that `comp_model` has some missing channels in the convolutional layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1610297665468,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "HW_7U8RUnwUe",
    "outputId": "6ec98696-025f-4b00-ea47-5bc1467b558f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************************\n",
      "Compressed Model Statistics\n",
      "Baseline model accuracy: 0.844383, Compressed model accuracy: 0.845304\n",
      "Compression ratio for memory=0.972344, mac=0.427639\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Per-layer Stats\n",
      "    Name:convs.3, compression-ratio: 0.4\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "Greedy Eval Dict\n",
      "    Layer: convs.3\n",
      "        Ratio=0.1, Eval score=0.8186003683241252\n",
      "        Ratio=0.2, Eval score=0.819060773480663\n",
      "        Ratio=0.3, Eval score=0.8176795580110497\n",
      "        Ratio=0.4, Eval score=0.8462246777163904\n",
      "        Ratio=0.5, Eval score=0.8457642725598526\n",
      "        Ratio=0.6, Eval score=0.8471454880294659\n",
      "        Ratio=0.7, Eval score=0.8448434622467772\n",
      "        Ratio=0.8, Eval score=0.8448434622467772\n",
      "        Ratio=0.9, Eval score=0.8448434622467772\n",
      "\n",
      "**********************************************************************************************\n",
      "\n",
      "---------- Original model ----------\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "---------- Compressed model ----------\n",
      "LeNet5(\n",
      "  (convs): Sequential(\n",
      "    (0): Conv2d(1, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(stats_prun)\n",
    "print('-'*10 + ' Original model ' + '-'*10)\n",
    "print(model)\n",
    "print('-'*10 + ' Compressed model ' + '-'*10)\n",
    "print(comp_model_prun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY1XFsmrpE2Y"
   },
   "source": [
    "Now let's see how they compare in terms of speed.\n",
    "Differences probably won't be huge here since the network is quite small to begin with, and only a single layer is pruned, but the point is to see that the compressed one is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55985,
     "status": "ok",
     "timestamp": 1610297835445,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "Kb83uXumopad",
    "outputId": "fe194811-5adc-410a-c261-687638c883e7"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9cf77e6acb7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc_full = eval_callback(model, None, use_cuda=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc_comp = eval_callback(comp_model_prun, None, use_cuda=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d2a14d951043>\u001b[0m in \u001b[0;36meval_callback\u001b[0;34m(model, iterations, use_cuda)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m             raise RuntimeError(\n\u001b[1;32m    195\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "%timeit acc_full = eval_callback(model, None, use_cuda=True)\n",
    "%timeit acc_comp = eval_callback(comp_model_prun, None, use_cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSJJNo9VeXJV"
   },
   "source": [
    "# Noa's try at Spatial SVD with AIMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141074,
     "status": "ok",
     "timestamp": 1610298010127,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "zpzXKE-UeNUd",
    "outputId": "5b264c06-3e9e-4fac-821d-8bc89182c6e6"
   },
   "outputs": [],
   "source": [
    "# Import the needed package\n",
    "from aimet_torch.defs import SpatialSvdParameters\n",
    "greedy_params = GreedySelectionParameters(target_comp_ratio=Decimal(0.5))\n",
    "# Do not exclude anything in contrary to channel pruning\n",
    "modules_to_ignore = []\n",
    "auto_params = SpatialSvdParameters.AutoModeParams(greedy_params, modules_to_ignore)\n",
    "input_shape = (1, 1, 28, 28)\n",
    "# Delete all the parameters that don't give an error in channel pruning, but do here\n",
    "spatial_svd_params = SpatialSvdParameters(mode=SpatialSvdParameters.Mode.auto,\n",
    "                                                      params=auto_params)\n",
    "\n",
    "# This takes a bit\n",
    "comp_model_svd, stats_svd = ModelCompressor.compress_model(model,\n",
    "                                                   input_shape=input_shape,\n",
    "                                                   eval_callback=eval_callback,\n",
    "                                                   eval_iterations=None,\n",
    "                                                   compress_scheme=CompressionScheme.spatial_svd,\n",
    "                                                   cost_metric=CostMetric.mac,\n",
    "                                                   parameters=spatial_svd_params,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u_PdEczhLN9"
   },
   "source": [
    "Look at the two models, but now compare it to the Spatial SVD one. You can see that the convolutional layers are split in two. What kind of effect this specificaly has, isn't clear yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1345,
     "status": "ok",
     "timestamp": 1610298036084,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "LzCGSsYnhH2W",
    "outputId": "14ce1e8f-98fb-4bd3-d293-90a715b38075"
   },
   "outputs": [],
   "source": [
    "print(stats_svd)\n",
    "print('-'*10 + ' Original model ' + '-'*10)\n",
    "print(model)\n",
    "print('-'*10 + ' Compressed model ' + '-'*10)\n",
    "print(comp_model_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUhBcSsTiPp9"
   },
   "source": [
    "Compare the speed difference between original model and this new compressed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57089,
     "status": "ok",
     "timestamp": 1610298249390,
     "user": {
      "displayName": "Jasper Mulder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GifrN-KID4f3Eu2i34s6o2boCZLCNEhKDJt_6esOw=s64",
      "userId": "07619433414547634568"
     },
     "user_tz": -60
    },
    "id": "9WuuBXMBiT7Q",
    "outputId": "d452bd0e-3bb0-4594-b947-08c956a19b11"
   },
   "outputs": [],
   "source": [
    "%timeit acc_full = eval_callback(model, None, use_cuda=True)\n",
    "%timeit acc_comp = eval_callback(comp_model_svd, None, use_cuda=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "our_aimet_ready.ipynb",
   "provenance": [
    {
     "file_id": "1IOgTNChF3Gmc_zEfY-3DpxcbeSCDxaUd",
     "timestamp": 1609924035770
    },
    {
     "file_id": "1_U4qdGKGRxnjczFdBOrN1O6AbisFxFTc",
     "timestamp": 1607621041273
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "012c3c9adc694baba6feb904d60f9201": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b5ccd5354b040b58f645351f4d950f3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b0a62dffc4341069d860e1e7c09a85c",
      "value": 1
     }
    },
    "0b0a62dffc4341069d860e1e7c09a85c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "154e97cc69984a0ab11ced26576ebbd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab3eb90c487746e8b65e62d7757c0e78",
      "placeholder": "",
      "style": "IPY_MODEL_c41636ad0caf4621870bd12e89518604",
      "value": " 32768/? [00:00&lt;00:00, 96064.27it/s]"
     }
    },
    "15b6b7d213c94345b6dedf59e8e065dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15c6e5135ed849bc89dec5bf0a38363b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab99643118ed466298c1ea12d4ac53d9",
      "placeholder": "",
      "style": "IPY_MODEL_c9dc0f6d46b74558bcb4962e3f1fb489",
      "value": " 0/? [00:00&lt;?, ?it/s]"
     }
    },
    "1af67330e5d947718a1e80e982898052": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2328a7b009d44c5e9eacff2a755cbebb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_46c25a22208a49eda4e6eab32d4eaf2f",
       "IPY_MODEL_d4ff65d7c61d4bf9b73c03e29c28a8ad"
      ],
      "layout": "IPY_MODEL_ed9207ce942047e9ae6072508f523741"
     }
    },
    "2a5ab588d2f24b0ab3ec8ce388f5127f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3b5ccd5354b040b58f645351f4d950f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46c25a22208a49eda4e6eab32d4eaf2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5f8150fe3124fc88dc714b404006f34",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a5ab588d2f24b0ab3ec8ce388f5127f",
      "value": 1
     }
    },
    "4b07de9e194f4439baeae01257e4ba52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7989862673284a799c9d51e431373247",
       "IPY_MODEL_154e97cc69984a0ab11ced26576ebbd7"
      ],
      "layout": "IPY_MODEL_bd10cc47f292451d922d8980f5e93ddf"
     }
    },
    "543c9bdec83f486eaa8b9b2b58ef211a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5503b3b0dbb548589aaa0279b976a414": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6126bcc00c54407688d7d3a081fa43c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1af67330e5d947718a1e80e982898052",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_657579abb9c4488c8f1a810a679dfda9",
      "value": 0
     }
    },
    "657579abb9c4488c8f1a810a679dfda9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7989862673284a799c9d51e431373247": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efa8ccf950004549a00250f6332bc16b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5503b3b0dbb548589aaa0279b976a414",
      "value": 1
     }
    },
    "8ce0005792b74005a30cb426b5c8077a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fffcd6e9d7a449fb4bfccedc6cb70a1",
      "placeholder": "",
      "style": "IPY_MODEL_543c9bdec83f486eaa8b9b2b58ef211a",
      "value": " 1654784/? [00:00&lt;00:00, 6376640.54it/s]"
     }
    },
    "8da6d53cc36743e6bd5058b8840e0c41": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fffcd6e9d7a449fb4bfccedc6cb70a1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5f8150fe3124fc88dc714b404006f34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab3eb90c487746e8b65e62d7757c0e78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab99643118ed466298c1ea12d4ac53d9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac82f07b477b4502b21ebaa8a539c4a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af3a484183a24008a6d38960f73ffb3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_012c3c9adc694baba6feb904d60f9201",
       "IPY_MODEL_8ce0005792b74005a30cb426b5c8077a"
      ],
      "layout": "IPY_MODEL_ac82f07b477b4502b21ebaa8a539c4a3"
     }
    },
    "bd10cc47f292451d922d8980f5e93ddf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c41636ad0caf4621870bd12e89518604": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9dc0f6d46b74558bcb4962e3f1fb489": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4ff65d7c61d4bf9b73c03e29c28a8ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6769ba59c21406e8c259661bdbf68f1",
      "placeholder": "",
      "style": "IPY_MODEL_8da6d53cc36743e6bd5058b8840e0c41",
      "value": " 9920512/? [00:20&lt;00:00, 27818522.57it/s]"
     }
    },
    "df98becb207b42f2bbeaf23708e5be02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6126bcc00c54407688d7d3a081fa43c0",
       "IPY_MODEL_15c6e5135ed849bc89dec5bf0a38363b"
      ],
      "layout": "IPY_MODEL_15b6b7d213c94345b6dedf59e8e065dc"
     }
    },
    "ed9207ce942047e9ae6072508f523741": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efa8ccf950004549a00250f6332bc16b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6769ba59c21406e8c259661bdbf68f1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
